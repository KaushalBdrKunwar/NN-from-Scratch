{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "VOo-92VVJ3B6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "_ZgBwiG8Kojv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42) # To generate the random number(Ensures Reproducibility)\n",
        "X_ROW, X_COLUMN =[2,2000] # 2 features(rows) and 2000 samples (columns) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Generate Raw Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### i. Creating Input Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0RZsW_G0vmP",
        "outputId": "e5747dea-500d-4d44-ccb1-e925e85b14b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[37.45401188, 95.07143064, 73.19939418, ...,  6.89580164,\n",
              "         5.70547212, 28.21870747],\n",
              "       [26.17056837, 24.69787991, 90.62545805, ..., 39.45723153,\n",
              "        52.99405869, 16.13673584]], shape=(2, 2000))"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_RAW =np.random.rand(X_ROW, X_COLUMN)*100  # 2*2000 matrix that has values between 0-100\n",
        "X_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ii. Creating Output Data(Y_Raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5cZ8o2y0vz3",
        "outputId": "e55086d5-0e62-4b78-ec3a-732d4c1b7147"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 63.62458026, 119.76931055, 163.82485223, ...,  46.35303317,\n",
              "         58.69953081,  44.35544331],\n",
              "       [ 11.28344351,  70.37355073, -17.42606387, ..., -32.5614299 ,\n",
              "        -47.28858658,  12.08197163],\n",
              "       [ 11.28344351,  70.37355073,  17.42606387, ...,  32.5614299 ,\n",
              "         47.28858658,  12.08197163]], shape=(3, 2000))"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_Raw =np.concatenate(([(X_RAW[0,:])+(X_RAW[1,:])],[(X_RAW[0,:])-(X_RAW[1,:])],np.abs([(X_RAW[0,:])-(X_RAW[1,:])])))\n",
        "Y_Raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1-NhAch6Zcz",
        "outputId": "706c4f7c-00e4-4f3f-d49c-be463e83432e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2000)"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_Row, Y_Col = Y_Raw.shape\n",
        "Y_Raw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Train_Test_Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "uTiE59Ak6ZfK"
      },
      "outputs": [],
      "source": [
        "# Training 70% - Testing 30%\n",
        "Train_rato= 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "IYZrphUR6Zim"
      },
      "outputs": [],
      "source": [
        "Num_data_train = int(Train_rato*X_COLUMN)\n",
        "X_RAW_Train = X_RAW[:,0:Num_data_train]\n",
        "X_RAW_Test = X_RAW[:,Num_data_train:]\n",
        "Y_RAW_Train = Y_Raw[:,0:Num_data_train]\n",
        "Y_RAW_Test = Y_Raw[:,Num_data_train:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Standardization(Scaling Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "vaS3s7jB0v2q"
      },
      "outputs": [],
      "source": [
        "class scalar: #stores the mean and std of each feature(rows)\n",
        "  def __init__(self,mean,std):\n",
        "    self.mean = mean\n",
        "    self.std = std if std != 0 else 1 #Avoid division by zero.\n",
        "\n",
        "def get_scalar(row): #Calculates mean and std.\n",
        "  mean = np.mean(row)\n",
        "  std =np.std(row)\n",
        "  return scalar(mean,std) # returns instance of scalar class\n",
        "\n",
        "def standarize(data,scalar):\n",
        "  return(data-(scalar.mean)/scalar.std)\n",
        "def unstandarize(data,scalar):\n",
        "  return(data*scalar.std) + scalar.mean\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Applying Standardization on train test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "CQEAV_060v5q"
      },
      "outputs": [],
      "source": [
        "X_scalers =[get_scalar(X_RAW_Train[row,:]) for row in range(X_ROW)]\n",
        "X_Train = np.array([standarize(X_RAW_Train[row,:],X_scalers[row])for row in range(X_ROW)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "-AebECRy0v9L"
      },
      "outputs": [],
      "source": [
        "Y_scalers =[get_scalar(Y_RAW_Train[row,:]) for row in range(Y_Row)]\n",
        "Y_Train = np.array([standarize(Y_RAW_Train[row,:],Y_scalers[row])for row in range(Y_Row)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "AmZ50NJcVwiZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.array([standarize(X_RAW_Test[row,:],X_scalers[row])for row in range(X_ROW)])\n",
        "Y_test = np.array([standarize(Y_RAW_Test[row,:],Y_scalers[row])for row in range(Y_Row)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jlRvCexWTS1",
        "outputId": "d96989eb-6aac-4014-983d-55a15c4b7c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[np.float64(48.11746678827335), np.float64(48.07886978975062)]\n"
          ]
        }
      ],
      "source": [
        "print([X_Train[row,:].mean() for row in range(X_ROW)]) #Verifying "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpH8EYJyeDCM",
        "outputId": "5d0994fc-5874-428a-af75-a76c197c7d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[np.float64(29.387072624266093), np.float64(28.779683508663084)]\n"
          ]
        }
      ],
      "source": [
        "print([X_Train[row,:].std() for row in range(X_ROW)]) #Verifying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyxmKgI7eDEK",
        "outputId": "884ba4f1-98a6-49f4-9c27-9f5df07ea728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[np.float64(97.20783555165191), np.float64(0.0028536514424484746), np.float64(32.078745112385825)]\n",
            "[np.float64(41.263859795310545), np.float64(41.00041847784436), np.float64(23.645246722265313)]\n"
          ]
        }
      ],
      "source": [
        "print([Y_Train[row,:].mean() for row in range(Y_Row)])#Verifying\n",
        "print([Y_Train[row,:].std() for row in range(Y_Row)])#Verifying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.Neural Network Construction\n",
        "##### It consists of Layer using W = Weight Matrix, b = bais vector, Neural net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### i. Defining the Neural Network Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "Gz83_fzcecNJ"
      },
      "outputs": [],
      "source": [
        "#This layer class is used to define a single layer in a neural network.\n",
        "#layer_index - Identifies which layer this is (e.g., input layer = 0, hidden layer = 1, etc.).\n",
        "# is_output - Boolean; True if this is the output layer.\n",
        "# input_dim - Number of neurons in the previous layer.\n",
        "# output_dim - Number of neurons in this layer.\n",
        "# activation - Activation function to be applied (e.g., ReLU, Sigmoid).\n",
        "class layer:\n",
        "  def __init__(self,layer_index,is_output,input_dim,output_dim,activation):\n",
        "    self.layer_index =layer_index\n",
        "    self.is_output = is_output\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.activation = activation\n",
        "    self.W = np.random.randn(output_dim, input_dim) * 0.01  # Weight initialization\n",
        "    self.b = np.zeros((output_dim, 1))  # Bias initialization\n",
        "    self.dW = np.zeros_like(self.W)  # Gradient for W (initialized to 0)\n",
        "    self.db = np.zeros_like(self.b)  # Gradient for b (initialized to 0)\n",
        "\n",
        "#Initializing Weights and Biases for layers except input layer.\n",
        "# formula W^l = randn(o/p_dim,ip_dim)/sqrt(2/ip_dim)\n",
        "    if layer_index != 0:\n",
        "      self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim)\n",
        "      self.b = np.random.randn(output_dim, 1)* np.sqrt(2/input_dim) #(np.sqrt(2/i/p dim) is he initialization, good for RelU-based networks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ii. Building the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "yeM47-k-eDHo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted number of hyperparameters: 47\n",
            "Actual Number of hyperparameters: 47\n",
            "Number of data: 2000\n"
          ]
        }
      ],
      "source": [
        "layers_dim = (X_ROW, 4, 4, Y_Row)  # Define the structure of the neural network(ip-hidden-hidden-output)\n",
        "Neural_Net = []\n",
        "\n",
        "for layer_index in range(len(layers_dim)):\n",
        "    if layer_index == 0:  # Input layer (No weights needed)\n",
        "        Neural_Net.append(layer(layer_index, False, 0, layers_dim[layer_index], 'irrelevant'))\n",
        "    elif layer_index + 1 == len(layers_dim):  # Output layer\n",
        "        Neural_Net.append(layer(layer_index, True, layers_dim[layer_index - 1], layers_dim[layer_index], activation='linear'))\n",
        "    else:  # Hidden layers\n",
        "        Neural_Net.append(layer(layer_index, False, layers_dim[layer_index - 1], layers_dim[layer_index], activation='relu'))\n",
        "\n",
        "# Predicted number of trainable parameters (weights + biases)\n",
        "pred_n_param = sum([(layers_dim[layer_index] + 1) * layers_dim[layer_index + 1] for layer_index in range(len(layers_dim) - 1)])\n",
        "\n",
        "# Actual number of parameters from the Neural_Net\n",
        "act_n_param = sum([Neural_Net[layer_index].W.size + Neural_Net[layer_index].b.size for layer_index in range(1, len(layers_dim))])\n",
        "\n",
        "print(f'Predicted number of hyperparameters: {pred_n_param}')\n",
        "print(f'Actual Number of hyperparameters: {act_n_param}')\n",
        "print(f'Number of data: {X_COLUMN}')\n",
        "\n",
        "if act_n_param >= X_COLUMN:\n",
        "    raise Exception(\"It will overfit.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 5.Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Forward Propagation:\n",
        "#Formula: a^l = (sigma)^l(z^l)= (W^l)^Ta^(l-1) + b^l i.e. Z = W * A_prev +b\n",
        "def activation(input_,act_function):\n",
        "    if act_function == 'relu':\n",
        "        return np.maximum(input_,np.zeros(input_.shape))\n",
        "    elif act_function == 'linear':\n",
        "        return input_\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined')\n",
        "\n",
        "def forward_prop(input_vec, layers_dim = layers_dim, Neural_Net = Neural_Net):\n",
        "    Neural_Net[0].A = input_vec\n",
        "    for layer_index in range(1,len(layers_dim)):\n",
        "        Neural_Net[layer_index].Z = np.add(np.dot(Neural_Net[layer_index].W, Neural_Net[layer_index-1].A), Neural_Net[layer_index].b)\n",
        "        Neural_Net[layer_index].A = activation(Neural_Net[layer_index].Z, Neural_Net[layer_index].activation)\n",
        "    return Neural_Net[layer_index].A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 6.Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Formula: az^l = {a^l-y if l= L and e is MSE\n",
        "#     (W^(l+1))^T(delta)z^(l+1) . sigma'(z^l) if l<L}\n",
        "#      NOW use deltaW^l= delta z^l(a^(l-1)^T) and deltab^l=deltaz^l\n",
        "\n",
        "def get_loss(Y, Y_Hat, metric='mse'):\n",
        "    if metric == 'mse':\n",
        "        individual_loss = 0.5 * (Y_Hat - Y) ** 2\n",
        "        loss = np.mean([np.linalg.norm(individual_loss[:, col], 2) for col in range(individual_loss.shape[1])])  \n",
        "        return loss  #  Ensure the function returns a value\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined')\n",
        "\n",
        "def get_dZ_from_loss(Y,Y_HAT,metric):\n",
        "    if metric == 'mse':\n",
        "        return Y_HAT - Y\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined')\n",
        "\n",
        "def get_deactivation(A, act_function):\n",
        "    if act_function == 'relu':\n",
        "        return np.maximum(np.sign(A),np.zeros(A.shape))\n",
        "    elif act_function == 'linear':\n",
        "        return np.ones(A.shape)\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined')\n",
        "\n",
        "def backward_prop(Y,Y_Hat, metric = 'mse', layers_dim = layers_dim, Neural_Net = Neural_Net, Num_data_train = Num_data_train ):\n",
        "    for layer_index in range(len(layers_dim)-1,0,1):\n",
        "        if layer_index +1 == len(layers_dim):\n",
        "            dZ = get_dZ_from_loss(Y,Y_Hat,metric)\n",
        "        else:\n",
        "            dZ = np.multiply(np.dot(Neural_Net[layer_index + 1].W.T, dZ),get_deactivation(Neural_Net[layer_index].A, Neural_Net[layer_index].activation))\n",
        "\n",
        "        dW = np.dot(dZ, Neural_Net[layer_index -1].A.T) / Num_data_train \n",
        "        db = np.sum(dZ,axis = 1, keepdims= True)/ Num_data_train \n",
        "\n",
        "        Neural_Net[layer_index].dW = dW\n",
        "        Neural_Net[layer_index].db = db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 5.Update Rules\n",
        "###### W = W - learning_rate * dW\n",
        "###### b = b - learning_rate * db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 6111.9770\n"
          ]
        }
      ],
      "source": [
        "#Epoch\n",
        "Learning_Rate = 0.001\n",
        "max_epoch = 1000000\n",
        "for epoch in range(1,max_epoch + 1):\n",
        "    Y_Hat_Train = forward_prop(X_Train)\n",
        "    backward_prop(Y_Train,Y_Hat_Train)\n",
        "    \n",
        "    for layer_index in range(1,len(layers_dim)):\n",
        "        Neural_Net[layer_index].W = Neural_Net[layer_index].W - Learning_Rate * Neural_Net[layer_index].dW\n",
        "        Neural_Net[layer_index].b = Neural_Net[layer_index].b - Learning_Rate * Neural_Net[layer_index].db\n",
        "    if epoch % 1000000 == 0:\n",
        "        print(f'{get_loss(Y_Train, Y_Hat_Train): .4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6081.700199535426\n"
          ]
        }
      ],
      "source": [
        "print(get_loss(Y_test, forward_prop(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def predict(X_Raw_Any):\n",
        "#     X_Any = np.array([standarize(X_Raw_Any[row,:],X_scalers[row]) for row in range(X_ROW)])\n",
        "#     Y_Hat = forward_prop(X_Any)\n",
        "#     Y_Hat_Any = np.array([standarize(X_Raw_Any[row,:],Y_scalers[row]) for row in range(Y_Row)])\n",
        "#     return Y_Hat_Any\n",
        "\n",
        "# predict(np.array([[30,70],[70,30],[3,5],[888,122]]).T)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
